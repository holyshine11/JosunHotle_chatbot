# Performance Optimization Plan

## 개요
- **목표**: RAG 챗봇 전체 응답 시간 단축
- **환경 제약**: 32GB M5 MacBook, macOS, Ollama qwen2.5:7b Q4 양자화, Metal GPU
- **핵심 원칙**: 정확도 유지하면서 체감 속도 최대화
- **달성**: FAQ 0.3~0.7s, LLM 7~14s (기존 15~35s 대비 50~98% 단축)

## 병목 분석 (현재 상태)

| 구간 | 이전 | 현재 | 개선율 | 현재 비중 |
|------|------|------|--------|----------|
| LLM: answerCompose | 5~15s | **7~13s** | 13~50%↓ | 85% |
| LLM: queryRewrite | 3~10s | **1.5~2.3s** | 50~77%↓ | 10% (멀티턴만) |
| Reranker | 200~1400ms | **200~700ms** | 50~86%↓ | 3% |
| 검색 (Vector+BM25) | 100~300ms | **20~50ms** | 80%↓ | <1% |
| 검증 파이프라인 | 10~50ms | **1~5ms** | 90%↓ | <1% |
| TTS (음성) | 1~3s | **0.7s (병렬)** | 91%↓ | 체감 0 |
| FAQ 직접 추출 | - | **0.3~0.7s** | LLM 완전 스킵 | - |

**총 합산 변화**: 싱글턴 15~35s → **0.3~14s**, 멀티턴 20~40s → **9~16s**

## 최적화 결과 (2026-02-18)

### 구현 완료

| 최적화 | 내용 | 효과 |
|--------|------|------|
| Ollama 설정 | num_ctx=4096, keep_alive=60m, num_thread=8 | cold start 제거 |
| FAQ 직접 추출 | score>=0.72 + Q매칭 시 LLM 스킵 | FAQ 응답 0.6~2s |
| 리랭커 최적화 | SKIP_THRESHOLD=0.90, 캐시 500개 | 캐시 히트 시 0ms |
| 동적 maxTokens | 짧은 질문 256, 복합 512 | 출력 토큰 절약 |
| SSE 단계별 피드백 | LangGraph stream + 4단계 상태 표시 | 체감 대기 감소 |
| TTS 병렬 생성 | 답변 생성 중 Edge TTS 동시 실행 | 7.8s → 0.7s (91%↓) |
| 실시간 토큰 스트리밍 | Ollama stream=True, answerCompose만 활성화 | TTFT 56%↓ |
| 프롬프트 경량화 | 시스템 639→216토큰, 유저 232→34토큰 (70%↓) | 총 시간 62%↓ |
| **MPS 리랭커 가속** | Metal GPU (Apple M5) 추론 | **1.4s → 0.2~0.7s (50~86%↓)** |
| **동의어 FAQ 매칭** | 다이닝↔dinner 등 동의어 확장 + 리랭커 신뢰도 임계값 | **19.2s → 0.7s (96%↓)** |
| **컨텍스트 축소** | 단순 싱글턴 질문 시 청크 5→3개 | LLM 입력 토큰 ~40% 절약 |
| **queryRewrite 경량화** | 프롬프트 300→100토큰, maxTokens 100→60, 히스토리 3→2턴 | **~5s → 1.5~2.3s (50%↓)** |

### 성능 비교 (warm 상태, 모델 로딩 제외)

| 케이스 | 최초 | 1차 최적화 | 2차 최적화 | 최종 개선 |
|--------|------|-----------|-----------|----------|
| FAQ 응답 (체크인) | 2.0s | 0.6~2.0s | **0.3s** | 85%↓ |
| FAQ 응답 (반려동물) | 2.0s | 0.6~2.0s | **0.3s** | 85%↓ |
| 다이닝 (LLM→FAQ 전환) | 35.1s | 19.2s | **0.7s** | **98%↓** |
| LLM (부대시설) | 35.9s | 13.5s | **13.1s** | 64%↓ |
| LLM (조식) | ~25s | ~15s | **7.6s** | 70%↓ |
| 리랭커 (warm) | 1.4s | 1.4s | **0.2~0.7s** | 50~86%↓ |
| queryRewrite (멀티턴) | 3~10s | 3~10s | **1.5~2.3s** | 50~77%↓ |
| TTS 준비 시간 | 7.8s | 0.7s | 0.7s | 91%↓ |

### 병목 분석 (최적화 후)

| 구간 | 소요 시간 | 비중 | 상태 |
|------|-----------|------|------|
| LLM: answerCompose | 7~13s | 85% | GPU 100% 활용 중, 추가 최적화 한계 |
| LLM: queryRewrite | 1.5~2.3s | 10% | 프롬프트 경량화 완료, 멀티턴만 |
| Reranker (MPS) | 0.2~0.7s | 3% | MPS 가속 완료 |
| 검색 (Vector+BM25) | 20~50ms | <1% | 최적 |
| 검증 파이프라인 | 1~5ms | <1% | 최적 |
| TTS (병렬) | 0.7s | - | 답변과 병렬, 체감 0 |

### 품질 변화

| 지표 | 이전 | 현재 | 변화 |
|------|------|------|------|
| 정확도 (66 QA) | 97.0% | **100.0%** | +3.0% |
| 멀티턴 (22턴) | 95% | **100%** | +5% |
| 할루시네이션 | 1.5% | **0.0%** | -1.5% |

### 미적용 (추후 검토)

| 항목 | 이유 |
|------|------|
| ONNX 리랭커 | MPS 가속으로 0.2~0.7s 달성, ONNX 불필요 |
| queryRewrite 규칙기반 | 프롬프트 경량화로 1.5~2.3s 달성, 규칙기반 리스크 불필요 |
| 경량 리랭커 (MiniLM) | 한국어 성능 저하 우려, MPS로 충분히 빠름 |
| float16 리랭커 | Apple M5 CPU에서 오히려 4배 느려짐 (fp16 에뮬레이션 오버헤드) |
| num_batch 증가 | 1024로 올리면 오히려 35% 느려짐 (512 최적) |
| MAX_LENGTH=256 | 긴 청크 잘림 → 리랭킹 품질 저하로 1건 회귀, 512 유지 |

## 최적화 전략 (6단계)

### Phase 1: LLM 호출 최적화 (영향도: ★★★★★)

#### 1-1. Ollama 설정 최적화
- `num_ctx` 축소: 32768 → 4096 (컨텍스트 윈도우 줄여서 메모리/속도 개선)
- `num_thread` 설정: CPU 코어수에 맞게 명시적 지정
- `keep_alive` 설정: 모델 메모리 상주 유지 (-1 = 영구)
- `num_gpu` 확인: Metal(Apple Silicon) GPU 오프로딩 가능 여부

#### 1-2. 프롬프트 경량화
- **answerCompose 시스템 프롬프트**: 현재 ~800토큰 → 400토큰으로 압축
  - 반복되는 금지 규칙 통합
  - 예시 제거 (이미 학습된 패턴)
  - 핵심 규칙만 유지
- **queryRewrite 프롬프트**: 현재 ~300토큰 → 150토큰으로 압축
- **maxTokens 동적 조절**: 짧은 질문 → maxTokens=256, 복합 질문 → 512

#### 1-3. queryRewrite LLM 호출 최소화
- **Rule-based 재작성 우선**: "그럼 X는?", "거기 Y는?" 패턴은 LLM 없이 규칙으로 처리
- **짧은 후속 질문 템플릿**: 세션 컨텍스트의 current_topic을 직접 삽입
- LLM은 복잡한 맥락 참조 (3턴 이상) 시에만 호출

#### 1-4. temperature=0.0 명시 (이미 적용됨, 확인)
- 이미 temperature=0.0 사용 중 → 그대로 유지

### Phase 2: 리랭커 속도 최적화 (영향도: ★★★☆☆)

#### 2-1. ONNX Runtime 변환
- bge-reranker-v2-m3를 ONNX 포맷으로 변환
- `onnxruntime` (CPU 최적화)으로 추론 → 2~3배 속도 향상 예상
- 변환 스크립트: `optimum` 라이브러리 사용

#### 2-2. 조건부 리랭킹 건너뛰기
- Vector 검색 top score > 0.90이면 리랭킹 스킵
- 검색 결과가 2개 미만이면 리랭킹 불필요 (이미 적용됨)
- 캐시 히트율이 높으면 리랭킹 스킵

#### 2-3. 캐시 크기 확대
- 현재 100개 → 500개로 확대
- 세션 내 유사 질문 반복 시 캐시 히트 극대화

#### 2-4. 경량 리랭커 대안 검토
- `bge-reranker-v2-m3` (560M) → `bge-reranker-base` (278M) 또는 `cross-encoder/ms-marco-MiniLM-L-6-v2` (22M)
- 한국어 성능 확인 필요 (다국어 모델 필수)

### Phase 3: TTS/음성 최적화 (영향도: ★★★☆☆)

#### 3-1. TTS 병렬 처리
- 답변 생성과 TTS를 병렬로 실행하지 않음 (현재: 답변 완료 후 TTS 호출)
- **프론트엔드에서 답변 수신 즉시 TTS 요청** → 체감 대기 감소

#### 3-2. TTS 청크 스트리밍
- 답변을 문장 단위로 분할하여 첫 문장 TTS부터 재생 시작
- 나머지 문장은 백그라운드에서 생성 + 이어 붙이기

#### 3-3. TTS 캐싱
- 자주 반복되는 답변 (호텔 연락처, 체크인 시간 등) 캐싱
- MD5(text) → MP3 파일 캐시 (메모리 또는 디스크)

#### 3-4. TTS 속도 파라미터 최적화
- 현재 rate="+0%" → rate="+10%" 로 약간 빠르게 (자연스러움 유지)

### Phase 4: 검색 파이프라인 최적화 (영향도: ★★☆☆☆)

#### 4-1. 서버 시작 시 Warm-up
- 현재: 첫 요청 시 모델 로딩 (cold start ~10s)
- 개선: `@app.on_event("startup")`에서 모든 모델 사전 로딩
  - Embedding 모델 (multilingual-e5-small)
  - Reranker 모델 (bge-reranker-v2-m3)
  - Ollama 모델 warm-up (더미 프롬프트 1회 호출)
  - Chroma DB 연결 확인

#### 4-2. Vector 검색 topK 동적 조절
- 단순 질문 (카테고리 명확) → topK=3
- 복합 질문 (카테고리 불명확) → topK=5 (현재 기본값)

### Phase 5: 파이프라인 구조 최적화 (영향도: ★★★★☆)

#### 5-1. 고신뢰 답변 검증 단축
- Grounding confidence="확실" + 수치 없음 → 세부 검증 스킵
- 카테고리 오염 검사: 대화 주제 없으면 스킵
- 교통편 할루시네이션 검사: 교통 관련 질문 아니면 스킵

#### 5-2. SSE 스트리밍 개선
- 현재: 전체 답변 생성 후 단어 단위 가짜 스트리밍
- 개선: 단계별 진행 상태 SSE 이벤트
  - `analyzing` → `searching` → `generating` → `verifying` → `done`
  - 각 단계 시작 시 즉시 프론트엔드에 상태 전달 → 체감 대기 감소

#### 5-3. answerCompose 직접 추출 우선
- top chunk score > 0.95 + FAQ 형식("Q:", "A:") → LLM 호출 없이 직접 추출
- 이미 _checkContextSufficiency에서 일부 구현됨 → 확대 적용

### Phase 6: 프론트엔드 체감 속도 (영향도: ★★★★☆)

#### 6-1. SSE 단계별 UI 피드백
- "질문을 분석하고 있습니다..." → "관련 정보를 검색합니다..." → "답변을 생성합니다..."
- 각 노드 진입 시점에 SSE 이벤트 전송

#### 6-2. 스켈레톤 UI
- 응답 대기 중 스켈레톤 로딩 표시
- 타이핑 인디케이터 (점 세 개 애니메이션)

#### 6-3. 예상 소요시간 표시 (선택적)
- "약 5초 후 답변이 준비됩니다" 표시

## 구현 우선순위 & 예상 효과

| 우선순위 | 항목 | 예상 단축 | 난이도 | 리스크 |
|----------|------|-----------|--------|--------|
| 1 | 1-1. Ollama 설정 최적화 | -3~5s | 낮음 | 없음 |
| 2 | 1-3. queryRewrite Rule-based 전환 | -3~10s | 중간 | 낮음 |
| 3 | 1-2. 프롬프트 경량화 | -1~3s | 낮음 | 낮음 |
| 4 | 4-1. 서버 Warm-up | -5~10s (첫 요청) | 낮음 | 없음 |
| 5 | 5-3. 고점수 FAQ 직접 추출 | -5~15s (해당 경우) | 중간 | 중간 |
| 6 | 2-1. ONNX 리랭커 | -200~400ms | 중간 | 낮음 |
| 7 | 5-2. SSE 단계별 피드백 | 체감 -50% | 중간 | 없음 |
| 8 | 3-1~3-4. TTS 최적화 | -1~2s | 중간 | 낮음 |
| 9 | 5-1. 검증 단축 | -20~50ms | 낮음 | 낮음 |

## 측정 방법

```bash
# 응답 시간 측정 (파이프라인 타이밍 로그)
python tests/evaluate.py --quick  # 10개 샘플 시간 측정

# 개별 노드 타이밍 확인
# 이미 각 노드에 [타이밍] 로그 존재 → 파싱하여 통계
```

## 제약 조건
- 정확도 100% (50/50 골든 QA) 반드시 유지
- 멀티턴 100% (22/22) 유지
- 할루시네이션율 0.0% 유지
- 추가 유료 서비스 사용 금지 (Ollama 로컬 유지)
- 16GB RAM 내에서 동작

## 비적용 사항 (현실적으로 불가능)
- GPU 서버 사용 (예산 제약)
- LLM 모델 변경 (qwen2.5:7b가 한국어 최적)
- vLLM/TGI 등 추론 서버 (로컬 16GB에서 오버헤드)
- 모델 양자화 추가 (이미 Q4 사용 중, 더 낮추면 품질 저하)
